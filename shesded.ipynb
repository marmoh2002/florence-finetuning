{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10122704,"sourceType":"datasetVersion","datasetId":6246396},{"sourceId":10123247,"sourceType":"datasetVersion","datasetId":6246788}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:26:56.344389Z","iopub.execute_input":"2024-12-06T20:26:56.345151Z","iopub.status.idle":"2024-12-06T20:26:56.748693Z","shell.execute_reply.started":"2024-12-06T20:26:56.345117Z","shell.execute_reply":"2024-12-06T20:26:56.747891Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cleaned-file/idk.json\n/kaggle/input/subset/idk.ipynb\n/kaggle/input/subset/images/images/cam_1532402933112460.jpg\n/kaggle/input/subset/images/images/cam_1532402935662460.jpg\n/kaggle/input/subset/images/images/cam_1532402938612460.jpg\n/kaggle/input/subset/images/images/cam_1533151614412404.jpg\n/kaggle/input/subset/images/images/cam_1533151621412404.jpg\n/kaggle/input/subset/images/images/cam_1533151620412404.jpg\n/kaggle/input/subset/images/images/cam_1533151609512404.jpg\n/kaggle/input/subset/images/images/cam_1532402943662460.jpg\n/kaggle/input/subset/images/images/cam_1532402941262460.jpg\n/kaggle/input/subset/images/images/cam_1533151619412404.jpg\n/kaggle/input/subset/images/images/cam_1532402934662484.jpg\n/kaggle/input/subset/images/images/cam_1532402940762460.jpg\n/kaggle/input/subset/images/images/cam_1533151613362404.jpg\n/kaggle/input/subset/images/images/cam_1532402929662460.jpg\n/kaggle/input/subset/images/images/cam_1532402942162460.jpg\n/kaggle/input/subset/images/images/cam_1533151619912458.jpg\n/kaggle/input/subset/images/images/cam_1533151603512404.jpg\n/kaggle/input/subset/images/images/cam_1532402945162460.jpg\n/kaggle/input/subset/images/images/cam_1532402938162460.jpg\n/kaggle/input/subset/images/images/cam_1532402939112460.jpg\n/kaggle/input/subset/images/images/cam_1533151607012404.jpg\n/kaggle/input/subset/images/images/cam_1533151613912404.jpg\n/kaggle/input/subset/images/images/cam_1532402929162460.jpg\n/kaggle/input/subset/images/images/cam_1533151618412404.jpg\n/kaggle/input/subset/images/images/cam_1532402928662460.jpg\n/kaggle/input/subset/images/images/cam_1533151615412404.jpg\n/kaggle/input/subset/images/images/cam_1532402946262460.jpg\n/kaggle/input/subset/images/images/cam_1532402937162460.jpg\n/kaggle/input/subset/images/images/cam_1533151606012404.jpg\n/kaggle/input/subset/images/images/cam_1533151612862404.jpg\n/kaggle/input/subset/images/images/cam_1532402932612460.jpg\n/kaggle/input/subset/images/images/cam_1532402940162460.jpg\n/kaggle/input/subset/images/images/cam_1533151608012404.jpg\n/kaggle/input/subset/images/images/cam_1532402945662460.jpg\n/kaggle/input/subset/images/images/cam_1533151611412404.jpg\n/kaggle/input/subset/images/images/cam_1532402931662460.jpg\n/kaggle/input/subset/images/images/cam_1533151604512404.jpg\n/kaggle/input/subset/images/images/cam_1533151610912404.jpg\n/kaggle/input/subset/images/images/cam_1532402928112460.jpg\n/kaggle/input/subset/images/images/cam_1533151609912404.jpg\n/kaggle/input/subset/images/images/cam_1533151605512404.jpg\n/kaggle/input/subset/images/images/cam_1533151617912404.jpg\n/kaggle/input/subset/images/images/cam_1533151622912404.jpg\n/kaggle/input/subset/images/images/cam_1532402935162463.jpg\n/kaggle/input/subset/images/images/cam_1533151609012404.jpg\n/kaggle/input/subset/images/images/cam_1533151612362404.jpg\n/kaggle/input/subset/images/images/cam_1532402944162460.jpg\n/kaggle/input/subset/images/images/cam_1533151614912404.jpg\n/kaggle/input/subset/images/images/cam_1532402937662460.jpg\n/kaggle/input/subset/images/images/cam_1532402931162460.jpg\n/kaggle/input/subset/images/images/cam_1532402934112460.jpg\n/kaggle/input/subset/images/images/cam_1533151610412404.jpg\n/kaggle/input/subset/images/images/cam_1532402930612460.jpg\n/kaggle/input/subset/images/images/cam_1532402936662460.jpg\n/kaggle/input/subset/images/images/cam_1532402936162460.jpg\n/kaggle/input/subset/images/images/cam_1532402941762460.jpg\n/kaggle/input/subset/images/images/cam_1533151622412404.jpg\n/kaggle/input/subset/images/images/cam_1533151604012404.jpg\n/kaggle/input/subset/images/images/cam_1532402932162460.jpg\n/kaggle/input/subset/images/images/cam_1533151616912404.jpg\n/kaggle/input/subset/images/images/cam_1532402939662460.jpg\n/kaggle/input/subset/images/images/cam_1532402942662460.jpg\n/kaggle/input/subset/images/images/cam_1532402946762460.jpg\n/kaggle/input/subset/images/images/cam_1533151605012404.jpg\n/kaggle/input/subset/images/images/cam_1533151618912404.jpg\n/kaggle/input/subset/images/images/cam_1532402944662460.jpg\n/kaggle/input/subset/images/images/cam_1533151615912404.jpg\n/kaggle/input/subset/images/images/cam_1533151608512404.jpg\n/kaggle/input/subset/images/images/cam_1532402927612460.jpg\n/kaggle/input/subset/images/images/cam_1532402933612460.jpg\n/kaggle/input/subset/images/images/cam_1533151617362404.jpg\n/kaggle/input/subset/images/images/cam_1533151611862404.jpg\n/kaggle/input/subset/images/images/cam_1532402943162460.jpg\n/kaggle/input/subset/images/images/cam_1533151621012404.jpg\n/kaggle/input/subset/images/images/cam_1532402930112460.jpg\n/kaggle/input/subset/images/images/cam_1533151616412404.jpg\n/kaggle/input/subset/images/images/cam_1533151607512404.jpg\n/kaggle/input/subset/images/images/cam_1533151606512404.jpg\n/kaggle/input/subset/images/images/cam_1533151621912404.jpg\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:26:56.750267Z","iopub.execute_input":"2024-12-06T20:26:56.750619Z","iopub.status.idle":"2024-12-06T20:27:06.755312Z","shell.execute_reply.started":"2024-12-06T20:26:56.750592Z","shell.execute_reply":"2024-12-06T20:27:06.754443Z"}},"outputs":[{"name":"stdout","text":"Collecting supervision\n  Downloading supervision-0.25.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: contourpy>=1.0.7 in /opt/conda/lib/python3.10/site-packages (from supervision) (1.2.1)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from supervision) (0.7.1)\nRequirement already satisfied: matplotlib>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from supervision) (3.7.5)\nRequirement already satisfied: numpy>=1.23.3 in /opt/conda/lib/python3.10/site-packages (from supervision) (1.26.4)\nRequirement already satisfied: opencv-python>=4.5.5.64 in /opt/conda/lib/python3.10/site-packages (from supervision) (4.10.0.84)\nRequirement already satisfied: pillow>=9.4 in /opt/conda/lib/python3.10/site-packages (from supervision) (10.3.0)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from supervision) (6.0.2)\nRequirement already satisfied: scipy<2.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from supervision) (1.14.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\nDownloading supervision-0.25.0-py3-none-any.whl (181 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: supervision\nSuccessfully installed supervision-0.25.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install einops\n!pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:27:06.756664Z","iopub.execute_input":"2024-12-06T20:27:06.756989Z","iopub.status.idle":"2024-12-06T20:27:23.624507Z","shell.execute_reply.started":"2024-12-06T20:27:06.756959Z","shell.execute_reply":"2024-12-06T20:27:23.623569Z"}},"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.8.0\nCollecting peft\n  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.46.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (1.1.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2024.6.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.0->peft) (2024.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.14.0-py3-none-any.whl (374 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.14.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":" # !mkdir -p /data/sets/nuscenes  # Make the directory to store the nuScenes dataset in.\n\n # !wget https://www.nuscenes.org/data/v1.0-mini.tgz  # Download the nuScenes mini split.\n\n # !tar -xf v1.0-mini.tgz -C /data/sets/nuscenes  # Uncompress the nuScenes mini split.\n\n # !pip install nuscenes-devkit &> /dev/null  # Install nuScenes.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:27:23.626839Z","iopub.execute_input":"2024-12-06T20:27:23.627147Z","iopub.status.idle":"2024-12-06T20:27:23.631618Z","shell.execute_reply.started":"2024-12-06T20:27:23.627119Z","shell.execute_reply":"2024-12-06T20:27:23.630701Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport torch.multiprocessing as mp\nimport argparse\nimport csv\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport sys\nfrom transformers import (\n    AdamW,\n    AutoModelForCausalLM,\n    AutoProcessor,\n    get_scheduler\n)\nfrom tqdm import tqdm\nfrom typing import List, Dict, Any, Tuple, Generator\nfrom peft import LoraConfig, get_peft_model\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:27:23.632701Z","iopub.execute_input":"2024-12-06T20:27:23.633038Z","iopub.status.idle":"2024-12-06T20:27:42.349499Z","shell.execute_reply.started":"2024-12-06T20:27:23.633003Z","shell.execute_reply":"2024-12-06T20:27:42.348807Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"directory_name = '/kaggle/working/datasett'\ntry:\n    os.mkdir(directory_name)\n    print(f\"Directory '{directory_name}' created successfully.\")\nexcept FileExistsError:\n    print(f\"Directory '{directory_name}' already exists.\")\nexcept PermissionError:\n    print(f\"Permission denied: Unable to create '{directory_name}'.\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:27:42.350651Z","iopub.execute_input":"2024-12-06T20:27:42.351299Z","iopub.status.idle":"2024-12-06T20:27:42.356689Z","shell.execute_reply.started":"2024-12-06T20:27:42.351256Z","shell.execute_reply":"2024-12-06T20:27:42.355810Z"}},"outputs":[{"name":"stdout","text":"Directory '/kaggle/working/datasett' created successfully.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# from transformers import AutoModelForCausalLM, AutoProcessor\n\nCHECKPOINT = \"microsoft/Florence-2-base-ft\"\nREVISION = 'refs/pr/6'\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    CHECKPOINT, trust_remote_code=True, revision=REVISION).to(DEVICE)\nprocessor = AutoProcessor.from_pretrained(\n    CHECKPOINT, trust_remote_code=True, revision=REVISION)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:27:42.358024Z","iopub.execute_input":"2024-12-06T20:27:42.358387Z","iopub.status.idle":"2024-12-06T20:28:00.913729Z","shell.execute_reply.started":"2024-12-06T20:27:42.358342Z","shell.execute_reply":"2024-12-06T20:28:00.913037Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/2.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4cf8e68033a4f7d99460f89cc141059"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_florence2.py:   0%|          | 0.00/15.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3ecb54bcd79477382e8cfac2e75fd21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modeling_florence2.py:   0%|          | 0.00/127k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9a73fd8c9e44b06aac41a002cc343fb"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/464M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b4db3ebd504bb6b543fdd74742d6f3"}},"metadata":{}},{"name":"stderr","text":"Florence2LanguageForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d85ee3d7c8e4b75b89d00c1b0fbe3ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processing_florence2.py:   0%|          | 0.00/46.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78272dea46f84ebba35419de6e69da4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/34.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d150e3b2ea74c8aabc6dd113a30d46f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c34870351344e46a59a8b0fd4782af1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae8b5d4c1a64eaeb51339b4b86e640b"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Florence-2-base-ft:\n- configuration_florence2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Test Inference","metadata":{}},{"cell_type":"code","source":"# import supervision as sv\n# from PIL import Image\n\n# image = Image.open(EXAMPLE_IMAGE_PATH)\n# task = \"<OD>\"\n# text = \"<OD>\"\n\n# inputs = processor(\n#     text=text, \n#     images=image, \n#     return_tensors=\"pt\"\n# ).to(DEVICE)\n# generated_ids = model.generate(\n#     input_ids=inputs[\"input_ids\"],\n#     pixel_values=inputs[\"pixel_values\"],\n#     max_new_tokens=1024,\n#     num_beams=3)\n# generated_text = processor.batch_decode(\n#     generated_ids, skip_special_tokens=False)[0]\n# response = processor.post_process_generation(\n#     generated_text, \n#     task=task, \n#     image_size=image.size)\n# detections = sv.Detections.from_lmm(\n#     sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\n# bounding_box_annotator = sv.BoundingBoxAnnotator(\n#     color_lookup=sv.ColorLookup.INDEX)\n# label_annotator = sv.LabelAnnotator(\n#     color_lookup=sv.ColorLookup.INDEX)\n\n# image = bounding_box_annotator.annotate(image, detections)\n# image = label_annotator.annotate(image, detections)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:00.914783Z","iopub.execute_input":"2024-12-06T20:28:00.915067Z","iopub.status.idle":"2024-12-06T20:28:00.919327Z","shell.execute_reply.started":"2024-12-06T20:28:00.915041Z","shell.execute_reply":"2024-12-06T20:28:00.918587Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# import supervision as sv\n# from PIL import Image\n\n# image = Image.open(EXAMPLE_IMAGE_PATH)\n# task = \"<CAPTION_TO_PHRASE_GROUNDING>\"\n# text = \"<CAPTION_TO_PHRASE_GROUNDING> In this image we can see a person wearing a bag and holding a dog. In the background there are buildings, poles and sky with clouds.\"\n\n# inputs = processor(\n#     text=text, \n#     images=image, \n#     return_tensors=\"pt\"\n# ).to(DEVICE)\n# generated_ids = model.generate(\n#     input_ids=inputs[\"input_ids\"],\n#     pixel_values=inputs[\"pixel_values\"],\n#     max_new_tokens=1024,\n#     num_beams=3)\n# generated_text = processor.batch_decode(\n#     generated_ids, skip_special_tokens=False)[0]\n# response = processor.post_process_generation(\n#     generated_text, \n#     task=task, \n#     image_size=image.size)\n# detections = sv.Detections.from_lmm(\n#     sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n\n# bounding_box_annotator = sv.BoundingBoxAnnotator(\n#     color_lookup=sv.ColorLookup.INDEX)\n# label_annotator = sv.LabelAnnotator(\n#     color_lookup=sv.ColorLookup.INDEX)\n\n# image = bounding_box_annotator.annotate(image, detections)\n# image = label_annotator.annotate(image, detections)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:00.920443Z","iopub.execute_input":"2024-12-06T20:28:00.920789Z","iopub.status.idle":"2024-12-06T20:28:00.932242Z","shell.execute_reply.started":"2024-12-06T20:28:00.920722Z","shell.execute_reply":"2024-12-06T20:28:00.931455Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nTARGET_MODULES = [\n    \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \n    \"linear\", \"Conv2d\", \"lm_head\", \"fc2\"\n]\n\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=8,\n    target_modules=TARGET_MODULES,\n    task_type=\"CAUSAL_LM\",\n    lora_dropout=0.05,\n    bias=\"none\",\n    inference_mode=False,\n    use_rslora=True,\n    init_lora_weights=\"gaussian\",\n    revision=REVISION\n)\n\npeft_model = get_peft_model(model, config)\npeft_model.print_trainable_parameters()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:00.935634Z","iopub.execute_input":"2024-12-06T20:28:00.935889Z","iopub.status.idle":"2024-12-06T20:28:01.090234Z","shell.execute_reply.started":"2024-12-06T20:28:00.935865Z","shell.execute_reply":"2024-12-06T20:28:01.089368Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,929,928 || all params: 272,733,896 || trainable%: 0.7076\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Dataset Formatting","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\n\ndef correct_format(image_dir, corrected_data_path):\n    # Load the corrected data\n    with open(corrected_data_path, 'r') as f:\n        corrected_data = json.load(f)\n\n    # Ensure the dataset is in the correct format\n    for entry in corrected_data:\n        image_path = os.path.join(image_dir,entry['filename'])\n        caption = entry['description']\n\n        # Extract the image file name from the image path\n        image_file_name = entry['filename']\n        json_file_name = os.path.splitext(image_file_name)[0] + '.json'\n        json_file_path = os.path.join(image_dir, json_file_name)\n        new_json_file_path = os.path.join('/kaggle/working/datasett', json_file_name)\n        filename = entry['filename'] # int(image_path.split('/')[-1].split('_')[-2])\n        # Split the filename by '_' and take the before last element\n        # Create the JSON content\n        json_content = {\n            \"questionId\": filename,\n            \"question\": \"Describe this image\",\n            \"answers\": caption\n        }\n        # Write the JSON content to the file\n        with open(new_json_file_path, 'w') as json_file:\n            json.dump(json_content, json_file, indent=4)\n    print(\"Dataset has been formatted correctly.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.091377Z","iopub.execute_input":"2024-12-06T20:28:01.091727Z","iopub.status.idle":"2024-12-06T20:28:01.098692Z","shell.execute_reply.started":"2024-12-06T20:28:01.091689Z","shell.execute_reply":"2024-12-06T20:28:01.097662Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"image_dir = '/kaggle/input/subset/images/images'\ncorrected_data_path = '/kaggle/input/cleaned-file/idk.json'\ncorrect_format(image_dir, corrected_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.099999Z","iopub.execute_input":"2024-12-06T20:28:01.100923Z","iopub.status.idle":"2024-12-06T20:28:01.124805Z","shell.execute_reply.started":"2024-12-06T20:28:01.100881Z","shell.execute_reply":"2024-12-06T20:28:01.123997Z"}},"outputs":[{"name":"stdout","text":"Dataset has been formatted correctly.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"### Downloading the formatted JSONS","metadata":{}},{"cell_type":"code","source":"# !zip -r file.zip /kaggle/working/jsons","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.125819Z","iopub.execute_input":"2024-12-06T20:28:01.126142Z","iopub.status.idle":"2024-12-06T20:28:01.130052Z","shell.execute_reply.started":"2024-12-06T20:28:01.126106Z","shell.execute_reply":"2024-12-06T20:28:01.129203Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"### Copying images to output directory","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source and destination directories\nsource_dir = '/kaggle/input/subset/images/images'\ndestination_dir = '/kaggle/working/datasett'\n\n# Create the destination directory if it doesn't exist\nos.makedirs(destination_dir, exist_ok=True)\n\n# Copy all image files from source to destination\nfor file_name in os.listdir(source_dir):\n    if file_name.endswith('.jpg') or file_name.endswith('.png'):\n        source_file = os.path.join(source_dir, file_name)\n        destination_file = os.path.join(destination_dir, file_name)\n        shutil.copy(source_file, destination_file)\n\nprint(\"Images have been copied successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:39.616249Z","iopub.execute_input":"2024-12-06T20:28:39.616615Z","iopub.status.idle":"2024-12-06T20:28:40.063667Z","shell.execute_reply.started":"2024-12-06T20:28:39.616584Z","shell.execute_reply":"2024-12-06T20:28:40.062812Z"}},"outputs":[{"name":"stdout","text":"Images have been copied successfully.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Create Dataset class\n","metadata":{}},{"cell_type":"code","source":"class JSONDataset:\n    def __init__(self, json_file_path: str, image_directory_path: str):\n        self.json_file_path = json_file_path\n        self.image_directory_path = image_directory_path\n        self.entries = self._load_entries()\n\n    def _load_entries(self) -> List[Dict[str, Any]]:\n        entries = []\n        with open(self.json_file_path, 'r') as file:\n            for line in file:\n                data = json.loads(line)\n                entries.append(data)\n        return entries\n\n    def __len__(self) -> int:\n        return len(self.entries)\n\n    def __getitem__(self, idx: int) -> Tuple[Image.Image, Dict[str, Any]]:\n        if idx < 0 or idx >= len(self.entries):\n            raise IndexError(\"Index out of range\")\n\n        entry = self.entries[idx]\n        image_path = os.path.join(self.image_directory_path, entry['image'])\n        try:\n            image = Image.open(image_path)\n            return (image, entry)\n        except FileNotFoundError:\n            raise FileNotFoundError(f\"Image file {image_path} not found.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:42.964705Z","iopub.execute_input":"2024-12-06T20:28:42.965128Z","iopub.status.idle":"2024-12-06T20:28:42.973146Z","shell.execute_reply.started":"2024-12-06T20:28:42.965095Z","shell.execute_reply":"2024-12-06T20:28:42.972104Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class DetectionDataset(Dataset):\n    def __init__(self, data):\n        self.dataset = data\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        question = \"<DocVQA>\" + example['question']\n        answers = example['answers']\n        if answers is None:\n            answers = [\"\"]  # Handle case where answers is None\n        elif isinstance(answers, dict):\n            answers = list(answers.values())  # Handle case where answers is a dictionary\n        elif not isinstance(answers, list):\n            answers = [str(answers)]  # Convert answers to a list of strings if not already\n\n        first_answer = answers[0] if answers else \"\"\n        image = example['image']\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n        return question, first_answer, image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:46.114416Z","iopub.execute_input":"2024-12-06T20:28:46.114722Z","iopub.status.idle":"2024-12-06T20:28:46.120805Z","shell.execute_reply.started":"2024-12-06T20:28:46.114695Z","shell.execute_reply":"2024-12-06T20:28:46.119933Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def load_local_dataset(folder_name):\n    data = []\n    for file_name in os.listdir(folder_name):\n        \n        if file_name.endswith('.json'):\n            base_name = os.path.splitext(file_name)[0]\n            with open(os.path.join(folder_name, file_name), 'r') as f:\n                entry = json.load(f)\n                question_id = entry['questionId']\n                image_file_name = f'{base_name}.jpg'\n                image_path = os.path.join(folder_name, image_file_name)\n                if os.path.exists(image_path):\n                    try:\n                        with open(image_path, 'rb') as img_file:\n                            image = Image.open(img_file).convert('RGB')\n                            entry['image'] = image\n                    except Exception as e:\n                        print(f\"Error loading image for questionId {question_id}: {e}\")\n                else:\n                    print(f\"Image file {image_file_name} not found.\")\n                data.append(entry)\n    return data\n\ndef split_data(dataset_folder='datasett', split_ratio=0.8, batch_size=2, num_workers=0):\n      data = load_local_dataset(dataset_folder)\n      # Check if all entries have images and count them\n      train_images_count = sum(1 for entry in data if 'image' in entry)\n      print(f\"Total images in the dataset: {train_images_count}\")\n\n      # Split dataset into training and validation sets\n      split_index = int(len(data) * split_ratio)\n      train_data = data[:split_index]\n      val_data = data[split_index:]\n\n      print(f\"Total images in the training set: {sum(1 for entry in train_data if 'image' in entry)}\")\n      print(f\"Total images in the validation set: {sum(1 for entry in val_data if 'image' in entry)}\")\n\n      # Create datasets and dataloaders\n      train_dataset = DetectionDataset(train_data)\n      val_dataset = DetectionDataset(val_data)\n\n      train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=lambda x: collate_fn(x, processor, DEVICE), num_workers=num_workers, shuffle=True)\n      val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=lambda x: collate_fn(x, processor, DEVICE), num_workers=num_workers)\n      return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:47.414148Z","iopub.execute_input":"2024-12-06T20:28:47.414650Z","iopub.status.idle":"2024-12-06T20:28:47.427927Z","shell.execute_reply.started":"2024-12-06T20:28:47.414596Z","shell.execute_reply":"2024-12-06T20:28:47.426419Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_loader, val_loader = split_data(dataset_folder='/kaggle/working/datasett', split_ratio=0.8, batch_size=2, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:48.411107Z","iopub.execute_input":"2024-12-06T20:28:48.411971Z","iopub.status.idle":"2024-12-06T20:28:49.142687Z","shell.execute_reply.started":"2024-12-06T20:28:48.411935Z","shell.execute_reply":"2024-12-06T20:28:49.141823Z"}},"outputs":[{"name":"stdout","text":"Total images in the dataset: 79\nTotal images in the training set: 63\nTotal images in the validation set: 16\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Splitting dataset","metadata":{}},{"cell_type":"code","source":"# os.mkdir(f\"{destination_dir}/valid\")\n# os.mkdir(f\"{destination_dir}/train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.704870Z","iopub.status.idle":"2024-12-06T20:28:01.705137Z","shell.execute_reply.started":"2024-12-06T20:28:01.705005Z","shell.execute_reply":"2024-12-06T20:28:01.705018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# @title Initiate `DetectionsDataset` and `DataLoader` for train and validation subsets\nBATCH_SIZE = 6\nNUM_WORKERS = 0\n\ndef collate_fn(batch, processor, device):\n    questions, answers, images = zip(*batch)\n    inputs = processor(text=list(questions), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n    return inputs, answers\n\n# train_dataset = DetectionDataset(\n#     json_file_path = f\"{destination_dir}/train/captions.json\",\n#     image_directory_path = f\"{destination_dir}/train/\"\n# )\n# val_dataset = DetectionDataset(\n#     json_file_path = f\"{destination_dir}/valid/captions.json\",\n#     image_directory_path = f\"{destination_dir}/valid/\"\n# )\n\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, num_workers=NUM_WORKERS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:52.636996Z","iopub.execute_input":"2024-12-06T20:28:52.637696Z","iopub.status.idle":"2024-12-06T20:28:52.642986Z","shell.execute_reply.started":"2024-12-06T20:28:52.637663Z","shell.execute_reply":"2024-12-06T20:28:52.642142Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Load dataset","metadata":{}},{"cell_type":"code","source":"def load_local_dataset(folder_name):\n    data = []\n    for file_name in os.listdir(folder_name):\n        if file_name.endswith('.json'):\n            base_name = os.path.splitext(file_name)[0]\n            with open(os.path.join(folder_name, file_name), 'r') as f:\n                entry = json.load(f)\n                # question_id = entry['questionId']\n                image_file_name = f'{base_name}.jpg'\n                image_path = os.path.join(folder_name, image_file_name)\n                if os.path.exists(image_path):\n                    try:\n                        with open(image_path, 'rb') as img_file:\n                            image = Image.open(img_file).convert('RGB')\n                            entry['image'] = image\n                    except Exception as e:\n                        print(f\"Error loading image for questionId {question_id}: {e}\")\n                else:\n                    print(f\"Image file {image_file_name} not found.\")\n                data.append(entry)\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:57.190665Z","iopub.execute_input":"2024-12-06T20:28:57.191524Z","iopub.status.idle":"2024-12-06T20:28:57.197782Z","shell.execute_reply.started":"2024-12-06T20:28:57.191492Z","shell.execute_reply":"2024-12-06T20:28:57.196900Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_model(train_loader, val_loader, model, processor, epochs, lr=1e-6):\n    optimizer = AdamW(model.parameters(), lr=lr)\n    num_training_steps = epochs * len(train_loader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    train_losses = []\n    val_losses = []\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for inputs, answers in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}/{epochs}\"):\n            input_ids = inputs[\"input_ids\"]\n            pixel_values = inputs[\"pixel_values\"]\n            labels = processor.tokenizer(\n                text=answers,\n                return_tensors=\"pt\",\n                padding=True,\n                return_token_type_ids=False\n            ).input_ids.to(DEVICE)\n            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n            loss = outputs.loss\n\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)\n        train_losses.append(avg_train_loss)\n        print(f\"Average Training Loss: {avg_train_loss}\")\n\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, answers in tqdm(val_loader, desc=f\"Validation Epoch {epoch + 1}/{epochs}\"):\n\n                input_ids = inputs[\"input_ids\"]\n                pixel_values = inputs[\"pixel_values\"]\n                labels = processor.tokenizer(\n                    text=answers,\n                    return_tensors=\"pt\",\n                    padding=True,\n                    return_token_type_ids=False\n                ).input_ids.to(DEVICE)\n\n                outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n\n                val_loss += loss.item()\n\n            avg_val_loss = val_loss / len(val_loader)\n            val_losses.append(avg_val_loss)\n            print(f\"Average Validation Loss: {avg_val_loss}\")\n\n        output_dir = f\"./model_checkpoints/epoch_{epoch+1}\"\n        os.makedirs(output_dir, exist_ok=True)\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n\n        # save_losses_and_plot(train_losses, val_losses)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:29:00.509274Z","iopub.execute_input":"2024-12-06T20:29:00.509644Z","iopub.status.idle":"2024-12-06T20:29:00.522131Z","shell.execute_reply.started":"2024-12-06T20:29:00.509607Z","shell.execute_reply":"2024-12-06T20:29:00.521184Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"train_model(train_loader, val_loader, model, processor, epochs=5, lr=1e-6)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:29:02.160060Z","iopub.execute_input":"2024-12-06T20:29:02.160741Z","iopub.status.idle":"2024-12-06T20:32:07.059868Z","shell.execute_reply.started":"2024-12-06T20:29:02.160710Z","shell.execute_reply":"2024-12-06T20:32:07.058818Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nTraining Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:29<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 5.089716359972954\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Validation Loss: 4.573293507099152\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:28<00:00,  1.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 4.821926414966583\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Validation Loss: 4.5259296000003815\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:29<00:00,  1.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 5.028356559574604\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Validation Loss: 4.492994159460068\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:31<00:00,  1.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 4.963309712707996\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Validation Loss: 4.473590701818466\n","output_type":"stream"},{"name":"stderr","text":"Training Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:32<00:00,  1.02s/it]\n","output_type":"stream"},{"name":"stdout","text":"Average Training Loss: 4.818405136466026\n","output_type":"stream"},{"name":"stderr","text":"Validation Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:04<00:00,  1.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Average Validation Loss: 4.4673155546188354\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"output_dir = f\"/kaggle/working/model_checkpoint\"\nos.makedirs(output_dir, exist_ok=True)\nmodel.save_pretrained(output_dir)\nprocessor.save_pretrained(output_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:56:20.564071Z","iopub.execute_input":"2024-12-06T20:56:20.564518Z","iopub.status.idle":"2024-12-06T20:56:24.138603Z","shell.execute_reply.started":"2024-12-06T20:56:20.564481Z","shell.execute_reply":"2024-12-06T20:56:24.137690Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":34},{"cell_type":"markdown","source":"## IDK REALLY","metadata":{}},{"cell_type":"markdown","source":"Ignore for now","metadata":{}},{"cell_type":"code","source":"# # @title Collect predictions\n\n# PATTERN = r'([a-zA-Z0-9 ]+ of [a-zA-Z0-9 ]+)<loc_\\d+>'\n\n# def extract_classes(dataset: DetectionDataset):\n#     class_set = set()\n#     for i in range(len(dataset.dataset)):\n#         image, data = dataset.dataset[i]\n#         suffix = data[\"suffix\"]\n#         classes = re.findall(PATTERN, suffix)\n#         class_set.update(classes)\n#     return sorted(class_set)\n\n# CLASSES = extract_classes(train_dataset)\n\n# targets = []\n# predictions = []\n\n# for i in range(len(val_dataset.dataset)):\n#     image, data = val_dataset.dataset[i]\n#     prefix = data['prefix']\n#     suffix = data['suffix']\n\n#     inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n#     generated_ids = model.generate(\n#         input_ids=inputs[\"input_ids\"],\n#         pixel_values=inputs[\"pixel_values\"],\n#         max_new_tokens=1024,\n#         num_beams=3\n#     )\n#     generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n\n#     prediction = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n#     prediction = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, prediction, resolution_wh=image.size)\n#     prediction = prediction[np.isin(prediction['class_name'], CLASSES)]\n#     prediction.class_id = np.array([CLASSES.index(class_name) for class_name in prediction['class_name']])\n#     prediction.confidence = np.ones(len(prediction))\n\n#     target = processor.post_process_generation(suffix, task='<OD>', image_size=image.size)\n#     target = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, target, resolution_wh=image.size)\n#     target.class_id = np.array([CLASSES.index(class_name) for class_name in target['class_name']])\n\n#     targets.append(target)\n#     predictions.append(prediction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.712580Z","iopub.status.idle":"2024-12-06T20:28:01.712902Z","shell.execute_reply.started":"2024-12-06T20:28:01.712721Z","shell.execute_reply":"2024-12-06T20:28:01.712734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run Inference","metadata":{}},{"cell_type":"code","source":"# def render_inline(image: Image.Image, resize=(128, 128)):\n#     \"\"\"Convert image into inline html.\"\"\"\n#     image.resize(resize)\n#     with io.BytesIO() as buffer:\n#         image.save(buffer, format='jpeg')\n#         image_b64 = str(base64.b64encode(buffer.getvalue()), \"utf-8\")\n#         return f\"data:image/jpeg;base64,{image_b64}\"\n\n\n# def render_example(image: Image.Image, response):\n#     try:\n#         detections = sv.Detections.from_lmm(sv.LMM.FLORENCE_2, response, resolution_wh=image.size)\n#         image = sv.BoundingBoxAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image.copy(), detections)\n#         image = sv.LabelAnnotator(color_lookup=sv.ColorLookup.INDEX).annotate(image, detections)\n#     except:\n#         print('failed to redner model response')\n#     return f\"\"\"\n# <div style=\"display: inline-flex; align-items: center; justify-content: center;\">\n#     <img style=\"width:256px; height:256px;\" src=\"{render_inline(image, resize=(128, 128))}\" />\n#     <p style=\"width:512px; margin:10px; font-size:small;\">{html.escape(json.dumps(response))}</p>\n# </div>\n# \"\"\"\n\n\n# def render_inference_results(model, dataset: DetectionDataset, count: int):\n#     html_out = \"\"\n#     count = min(count, len(dataset))\n#     for i in range(count):\n#         image, data = dataset.dataset[i]\n#         prefix = data['prefix']\n#         suffix = data['suffix']\n#         inputs = processor(text=prefix, images=image, return_tensors=\"pt\").to(DEVICE)\n#         generated_ids = model.generate(\n#             input_ids=inputs[\"input_ids\"],\n#             pixel_values=inputs[\"pixel_values\"],\n#             max_new_tokens=1024,\n#             num_beams=3\n#         )\n#         generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n#         answer = processor.post_process_generation(generated_text, task='<OD>', image_size=image.size)\n#         html_out += render_example(image, answer)\n\n#     display(HTML(html_out))\n\n# render_inference_results(peft_model, val_dataset, 4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:28:01.714274Z","iopub.status.idle":"2024-12-06T20:28:01.714544Z","shell.execute_reply.started":"2024-12-06T20:28:01.714408Z","shell.execute_reply":"2024-12-06T20:28:01.714422Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save Model","metadata":{}},{"cell_type":"code","source":"# os.mkdir('/kaggle/working/model')\n# os.mkdir('/kaggle/working/model/florence2-lora')\npeft_model.save_pretrained(\"/model/florence2-lora\")\nprocessor.save_pretrained(\"/model/florence2-lora/\")\n\n# !ls -la /content/florence2-lora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:56:56.093058Z","iopub.execute_input":"2024-12-06T20:56:56.093396Z","iopub.status.idle":"2024-12-06T20:56:56.369465Z","shell.execute_reply.started":"2024-12-06T20:56:56.093368Z","shell.execute_reply":"2024-12-06T20:56:56.368671Z"}},"outputs":[{"name":"stderr","text":"UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"!pip install inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:40:50.203526Z","iopub.execute_input":"2024-12-06T20:40:50.204408Z","iopub.status.idle":"2024-12-06T20:40:59.891023Z","shell.execute_reply.started":"2024-12-06T20:40:50.204371Z","shell.execute_reply":"2024-12-06T20:40:59.890010Z"}},"outputs":[{"name":"stderr","text":"RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: inference in /opt/conda/lib/python3.10/site-packages (0.29.2)\nRequirement already satisfied: aiortc<2.0.0,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from inference) (1.9.0)\nRequirement already satisfied: APScheduler<4.0.0,>=3.10.1 in /opt/conda/lib/python3.10/site-packages (from inference) (3.11.0)\nRequirement already satisfied: asyncua~=1.1.5 in /opt/conda/lib/python3.10/site-packages (from inference) (1.1.5)\nRequirement already satisfied: cython~=3.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (3.0.10)\nRequirement already satisfied: python-dotenv~=1.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (1.0.1)\nRequirement already satisfied: fastapi<0.111,>=0.100 in /opt/conda/lib/python3.10/site-packages (from inference) (0.110.3)\nRequirement already satisfied: numpy<=1.26.4 in /opt/conda/lib/python3.10/site-packages (from inference) (1.26.4)\nRequirement already satisfied: opencv-python<=4.10.0.84,>=4.8.1.78 in /opt/conda/lib/python3.10/site-packages (from inference) (4.10.0.84)\nRequirement already satisfied: pillow<11.0 in /opt/conda/lib/python3.10/site-packages (from inference) (10.3.0)\nRequirement already satisfied: prometheus-fastapi-instrumentator<=6.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (6.0.0)\nRequirement already satisfied: redis~=5.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (5.0.8)\nRequirement already satisfied: requests<3.0.0,>=2.32.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.32.3)\nRequirement already satisfied: rich~=13.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (13.0.1)\nRequirement already satisfied: supervision<=0.22.0,>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from inference) (0.22.0)\nRequirement already satisfied: pybase64~=1.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (1.0.2)\nRequirement already satisfied: scikit-image<=0.24.0,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from inference) (0.23.2)\nRequirement already satisfied: requests-toolbelt~=1.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (1.0.0)\nRequirement already satisfied: wheel<=0.45.0,>=0.38.1 in /opt/conda/lib/python3.10/site-packages (from inference) (0.43.0)\nRequirement already satisfied: setuptools>=70.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (70.0.0)\nRequirement already satisfied: networkx~=3.1 in /opt/conda/lib/python3.10/site-packages (from inference) (3.3)\nRequirement already satisfied: pydantic~=2.6 in /opt/conda/lib/python3.10/site-packages (from inference) (2.10.1)\nRequirement already satisfied: pydantic-settings~=2.2 in /opt/conda/lib/python3.10/site-packages (from inference) (2.6.1)\nRequirement already satisfied: openai<2.0.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from inference) (1.57.0)\nRequirement already satisfied: structlog<25.0.0,>=24.1.0 in /opt/conda/lib/python3.10/site-packages (from inference) (24.4.0)\nRequirement already satisfied: zxing-cpp~=2.2.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.2.0)\nRequirement already satisfied: boto3<=1.35.60 in /opt/conda/lib/python3.10/site-packages (from inference) (1.26.100)\nRequirement already satisfied: typing_extensions<=4.12.2,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from inference) (4.12.2)\nRequirement already satisfied: pydot~=2.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.0.0)\nRequirement already satisfied: shapely<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.0.6)\nRequirement already satisfied: tldextract~=5.1.2 in /opt/conda/lib/python3.10/site-packages (from inference) (5.1.3)\nRequirement already satisfied: packaging~=24.0 in /opt/conda/lib/python3.10/site-packages (from inference) (24.2)\nRequirement already satisfied: anthropic~=0.34.2 in /opt/conda/lib/python3.10/site-packages (from inference) (0.34.2)\nRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.2.3)\nRequirement already satisfied: pytest<9.0.0,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (8.3.3)\nRequirement already satisfied: tokenizers<=0.20.3,>=0.19.0 in /opt/conda/lib/python3.10/site-packages (from inference) (0.20.3)\nRequirement already satisfied: slack-sdk~=3.33.4 in /opt/conda/lib/python3.10/site-packages (from inference) (3.33.5)\nRequirement already satisfied: twilio~=9.3.7 in /opt/conda/lib/python3.10/site-packages (from inference) (9.3.8)\nRequirement already satisfied: httpx~=0.25.1 in /opt/conda/lib/python3.10/site-packages (from inference) (0.25.2)\nRequirement already satisfied: onnxruntime<1.20.0,>=1.15.1 in /opt/conda/lib/python3.10/site-packages (from inference) (1.19.2)\nRequirement already satisfied: nvidia-ml-py<13.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (11.495.46)\nRequirement already satisfied: docker<8.0.0,>=7.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (7.1.0)\nRequirement already satisfied: typer<=0.12.5,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from inference) (0.12.3)\nRequirement already satisfied: PyYAML~=6.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (6.0.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (4.66.4)\nRequirement already satisfied: py-cpuinfo~=9.0.0 in /opt/conda/lib/python3.10/site-packages (from inference) (9.0.0)\nRequirement already satisfied: aiohttp<=3.10.11,>=3.9.0 in /opt/conda/lib/python3.10/site-packages (from inference) (3.9.5)\nRequirement already satisfied: backoff~=2.2.0 in /opt/conda/lib/python3.10/site-packages (from inference) (2.2.1)\nRequirement already satisfied: dataclasses-json~=0.6.0 in /opt/conda/lib/python3.10/site-packages (from inference) (0.6.7)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (4.0.3)\nRequirement already satisfied: aioice<1.0.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (0.9.0)\nRequirement already satisfied: av<13.0.0,>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (12.3.0)\nRequirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (1.16.0)\nRequirement already satisfied: cryptography>=42.0.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (42.0.8)\nRequirement already satisfied: google-crc32c>=1.1 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (1.5.0)\nRequirement already satisfied: pyee>=9.0.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (12.1.1)\nRequirement already satisfied: pylibsrtp>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (0.10.0)\nRequirement already satisfied: pyopenssl>=24.0.0 in /opt/conda/lib/python3.10/site-packages (from aiortc<2.0.0,>=1.9.0->inference) (24.0.0)\nRequirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic~=0.34.2->inference) (4.4.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic~=0.34.2->inference) (1.9.0)\nRequirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from anthropic~=0.34.2->inference) (0.8.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic~=0.34.2->inference) (1.3.1)\nRequirement already satisfied: tzlocal>=3.0 in /opt/conda/lib/python3.10/site-packages (from APScheduler<4.0.0,>=3.10.1->inference) (5.2)\nRequirement already satisfied: aiofiles in /opt/conda/lib/python3.10/site-packages (from asyncua~=1.1.5->inference) (22.1.0)\nRequirement already satisfied: aiosqlite in /opt/conda/lib/python3.10/site-packages (from asyncua~=1.1.5->inference) (0.20.0)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from asyncua~=1.1.5->inference) (2.9.0.post0)\nRequirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from asyncua~=1.1.5->inference) (2024.1)\nRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from asyncua~=1.1.5->inference) (2.4.0)\nRequirement already satisfied: botocore<1.30.0,>=1.29.100 in /opt/conda/lib/python3.10/site-packages (from boto3<=1.35.60->inference) (1.29.165)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<=1.35.60->inference) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<=1.35.60->inference) (0.6.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json~=0.6.0->inference) (3.23.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json~=0.6.0->inference) (0.9.0)\nRequirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker<8.0.0,>=7.0.0->inference) (1.26.18)\nRequirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<0.111,>=0.100->inference) (0.37.2)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.1->inference) (2024.6.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.1->inference) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx~=0.25.1->inference) (3.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx~=0.25.1->inference) (0.14.0)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.20.0,>=1.15.1->inference) (15.0.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.20.0,>=1.15.1->inference) (24.3.25)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.20.0,>=1.15.1->inference) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime<1.20.0,>=1.15.1->inference) (1.13.3)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->inference) (2024.1)\nRequirement already satisfied: prometheus-client<1.0.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from prometheus-fastapi-instrumentator<=6.0.0->inference) (0.20.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic~=2.6->inference) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic~=2.6->inference) (2.27.1)\nRequirement already satisfied: pyparsing>=3 in /opt/conda/lib/python3.10/site-packages (from pydot~=2.0.0->inference) (3.1.2)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->inference) (2.0.0)\nRequirement already satisfied: pluggy<2,>=1.5 in /opt/conda/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->inference) (1.5.0)\nRequirement already satisfied: exceptiongroup>=1.0.0rc8 in /opt/conda/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->inference) (1.2.0)\nRequirement already satisfied: tomli>=1 in /opt/conda/lib/python3.10/site-packages (from pytest<9.0.0,>=8.0.0->inference) (2.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.0->inference) (3.3.2)\nRequirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from rich~=13.0.0->inference) (0.9.1)\nRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from rich~=13.0.0->inference) (2.18.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image<=0.24.0,>=0.19.0->inference) (1.14.1)\nRequirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image<=0.24.0,>=0.19.0->inference) (2.34.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image<=0.24.0,>=0.19.0->inference) (2024.5.22)\nRequirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image<=0.24.0,>=0.19.0->inference) (0.4)\nRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from supervision<=0.22.0,>=0.21.0->inference) (0.7.1)\nRequirement already satisfied: matplotlib>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from supervision<=0.22.0,>=0.21.0->inference) (3.7.5)\nRequirement already satisfied: opencv-python-headless>=4.5.5.64 in /opt/conda/lib/python3.10/site-packages (from supervision<=0.22.0,>=0.21.0->inference) (4.10.0.84)\nRequirement already satisfied: requests-file>=1.4 in /opt/conda/lib/python3.10/site-packages (from tldextract~=5.1.2->inference) (2.1.0)\nRequirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from tldextract~=5.1.2->inference) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers<=0.20.3,>=0.19.0->inference) (0.26.2)\nRequirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from twilio~=9.3.7->inference) (2.8.0)\nRequirement already satisfied: aiohttp-retry==2.8.3 in /opt/conda/lib/python3.10/site-packages (from twilio~=9.3.7->inference) (2.8.3)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<=0.12.5,>=0.9.0->inference) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<=0.12.5,>=0.9.0->inference) (1.5.4)\nRequirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.9.0->inference) (2.6.1)\nRequirement already satisfied: ifaddr>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aioice<1.0.0,>=0.9.0->aiortc<2.0.0,>=1.9.0->inference) (0.2.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.0->aiortc<2.0.0,>=1.9.0->inference) (2.22)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.19.0->inference) (2024.6.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision<=0.22.0,>=0.21.0->inference) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision<=0.22.0,>=0.21.0->inference) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision<=0.22.0,>=0.21.0->inference) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision<=0.22.0,>=0.21.0->inference) (1.4.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil->asyncua~=1.1.5->inference) (1.16.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference) (1.0.0)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->onnxruntime<1.20.0,>=1.15.1->inference) (10.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime<1.20.0,>=1.15.1->inference) (1.3.0)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"import os\nfrom inference import get_model\nfrom PIL import Image\nimport json\n\nlora_model = get_model(\"model-id/version-id\", api_key=\"KEY\")\n\nimage = Image.open(\"containers.png\")\nresponse = lora_model.infer(image)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T20:33:34.978595Z","iopub.execute_input":"2024-12-06T20:33:34.979350Z","iopub.status.idle":"2024-12-06T20:33:37.432008Z","shell.execute_reply.started":"2024-12-06T20:33:34.979313Z","shell.execute_reply":"2024-12-06T20:33:37.430734Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModelArtefactError\u001b[0m                        Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel-id/version-id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKEY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontainers.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m lora_model\u001b[38;5;241m.\u001b[39minfer(image)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/inference/models/utils.py:361\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_id, api_key, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(model_id, api_key\u001b[38;5;241m=\u001b[39mAPI_KEY, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Model:\n\u001b[0;32m--> 361\u001b[0m     task, model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ROBOFLOW_MODEL_TYPES[(task, model)](model_id, api_key\u001b[38;5;241m=\u001b[39mapi_key, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/inference/core/registries/roboflow.py:125\u001b[0m, in \u001b[0;36mget_model_type\u001b[0;34m(model_id, api_key)\u001b[0m\n\u001b[1;32m    118\u001b[0m api_data \u001b[38;5;241m=\u001b[39m get_roboflow_model_data(\n\u001b[1;32m    119\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m    120\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m    121\u001b[0m     endpoint_type\u001b[38;5;241m=\u001b[39mModelEndpointType\u001b[38;5;241m.\u001b[39mORT,\n\u001b[1;32m    122\u001b[0m     device_id\u001b[38;5;241m=\u001b[39mGLOBAL_DEVICE_ID,\n\u001b[1;32m    123\u001b[0m )\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mort\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelArtefactError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading model artifacts from Roboflow API.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# some older projects do not have type field - hence defaulting\u001b[39;00m\n\u001b[1;32m    127\u001b[0m project_task_type \u001b[38;5;241m=\u001b[39m api_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject-detection\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mModelArtefactError\u001b[0m: Error loading model artifacts from Roboflow API."],"ename":"ModelArtefactError","evalue":"Error loading model artifacts from Roboflow API.","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}